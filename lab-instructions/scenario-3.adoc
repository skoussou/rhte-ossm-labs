= Scenario 3 - Setup `ServiceMeshControlPlane` for Production
:toc:

The Travel Agency company has a running development environment. Now the company is heading to production. In this scenario you are going to set up Openshift Service Mesh for production use, deploy the travel applications in the Mesh, exposing the application for external access with TLS and fine tune parameters for production.

[NOTE]
====
The Lab Instructors have already created all production namespaces for your user as well as the Openshift users that will interact with the mesh (including roles and rolebindings) for the production environment. You will now proceed to create the Prod Service Mesh Setup.
====

== Production Requirements

Let us recall the initial requirements for production, which we discussed in scenario 1:

1. The _Development Team_ wants to be able to trace (every) request during development, and a sample of `20%` of traffic in Production
2. The _Development Teams_ wants to implement some resiliency to make the overall application more stable and reliable.
3. The _Product Team_ wants to see metrics around performance/usage (storing them for up to 1 week)
4. The _Security_ team wishes to enable mTLS in all _intramesh_ and _intermesh_ communications
5. The _Platform Team_ wants to centrally manage security

== User/Roles Mapping for the Production environment

For the purpose of managing, monitoring and troubleshooting the Production environment we need specific Enterprise Personas.

[cols="1,1,3,4"]
.Users created in `PROD` Environment
|===
| Username | Password | Enterprise Persona |  Responsible for Namespace

| emma | emma | Mesh Operator | `prod-istio-system`

| cristina | cristina | Travel Portal Domain Owner (Tech Lead)  | `prod-travel-portal`, `prod-travel-control`

| farid | farid | Travel Services Domain Owner (Tech Lead)  | `prod-travel-agency`

| craig | craig | Platform (Application Ops) Team  | `prod-travel-portal`, `prod-travel-control`, `prod-travel-agency`

| mus | mus | Product Owner | `prod-travel-portal`, `prod-travel-control`, `prod-travel-agency`

|===

== Task 1: Export Environment variables

[IMPORTANT]
====
As this is a multi-tenant cluster you should restrict use for this lab to the following namespaces associated with your user *`$LAB_PARTICIPANT_ID-prod-istio-system`*, *`$LAB_PARTICIPANT_ID-prod-travel-control`*, *`$LAB_PARTICIPANT_ID-prod-travel-portal`*, *`$LAB_PARTICIPANT_ID-prod-travel-agency`*

Now export the following in the terminal of your choice (see link:../README.adoc[Lab Information] for values)

* export CLUSTER_CONSOLE=https://console-openshift-console.apps.<CLUSTER_NAME>.<DOMAIN_NAME>/
* export CLUSTER_API=https://api.<CLUSTER_NAME>.<DOMAIN_NAME>:6443/
* export LAB_PARTICIPANT_ID=<FROM 2nd column of THE TABLE below `userx`>
* export OCP_DOMAIN=apps.<CLUSTER_NAME>.<DOMAIN_NAME>
* export SSO_CLIENT_SECRET=bcd06d5bdd1dbaaf81853d10a66aeb989a38dd51
====

[NOTE]
====
If you are running out of time and wish to complete the following lab sections in a single step execute
----
cd lab-3
./complete-lab-3.sh $OCP_DOMAIN $LAB_PARTICIPANT_ID
----
====

== Task 2: Install a basic Service Mesh Control Plane with an external Jaeger configuration

You are going to create a basic Service Mesh Control Plane for production. Regarding the Tracing configuration, `Red Hat Openshift Service Mesh (OSSM)` makes the following 2 suggestions on setting up tracing for the production environment. We will select `Option 2`, the _fully customized_ option for the production setup.

- Option 1: link:https://docs.openshift.com/container-platform/4.11/service_mesh/v2x/ossm-deploy-production.html#ossm-smcp-prod_ossm-architecture[Production distributed tracing platform deployment (minimal) -  via SMCP Resource]
- Option 2: link:https://docs.openshift.com/container-platform/4.11/service_mesh/v2x/ossm-reference-jaeger.html#ossm-deploying-jaeger-production_jaeger-config-reference[Production distributed tracing platform deployment (fully customized)]


Login as Mesh Operator (credentials: `emma/emma`) and run the `create-prod-smcp-1-tracing.sh` script. This will deploy the production `SMCP` resource *`$LAB_PARTICIPANT_ID-production`* and an external fully customized `Jaeger` instance in your lab user's Service Mesh controlplane namespace *`$LAB_PARTICIPANT_ID-prod-istio-system`*.

[source,shell]
----
cd lab-3
./login-as.sh emma
./create-prod-smcp-1-tracing.sh $LAB_PARTICIPANT_ID-prod-istio-system $LAB_PARTICIPANT_ID-production
----

* The `Jaeger Operator` will also create a `Jaeger Collector`, a `Jaeger Query` and an `Elastic Search` Deployment in your `$LAB_PARTICIPANT_ID-prod-istio-system` in order to collect and backup the traces.
+
This is the Jaeger Custom Resource applied:
+
----
kind: Jaeger
metadata:
  name: jaeger-small-production
spec:
  strategy: production <1>
  storage:
    type: elasticsearch <2>
    esIndexCleaner:
      enabled: true
      numberOfDays: 7 <3>
      schedule: '55 23 * * *'
    elasticsearch:
      nodeCount: 1 <4>
      storage:
        size: 1Gi <5>
      resources:  <6>
        requests:
          cpu: 200m
          memory: 1Gi
        limits:
          memory: 1Gi
      redundancyPolicy: ZeroRedundancy <7>
----
The applied `Jaeger` setup will ensure that:

** *(1)* Production focused setup is applied
** *(2)* Backed up for persistence by Elastic Search
** *(3)* With indexes deleted every 7 days
** *(4)* Elastic Search will be hosted on a single Elastic node
** *(5)* Total Elastic Search Index size will be _`1Gi`_
** *(6)* Resource for the node will be both requested and limited
** *(7)* Since a single node is setup redundancy of the indices will be set to `ZeroRedundancy`


* This is the SMCP Resource that is configured to use the external Jaeger instance:
+
----
apiVersion: maistra.io/v2
kind: ServiceMeshControlPlane
metadata:
  name: production
spec:
  security:
    dataPlane:
      automtls: true
      mtls: true
  tracing:
    sampling: 2000 <1>
    type: Jaeger
  general:
    logging:
      logAsJSON: true
  profiles:
    - default
  proxy:
    accessLogging:
      file:
        name: /dev/stdout
    networking:
      trafficControl:
        inbound: {}
        outbound:
          policy: REGISTRY_ONLY <2>
  policy:
    type: Istiod
  addons:
    grafana:
      enabled: true
    jaeger:  <3>
      install:
        ingress:
          enabled: true
        storage:
          type: Elasticsearch <4>
      name: jaeger-small-production <5>
    kiali:
      enabled: true
    prometheus:
      enabled: true
  version: v2.2
  telemetry:
    type: Istiod"
----
+

The applied `ServiceMeshControlPlane` Resource ensures that:

** *(1)* 20% of all traces (as requested by the developers) will be collected,
** *(2)* No external outgoing communications to a host not registered in the mesh will be allowed,
** *(3)* `Jaeger` resource will be available in the `Service Mesh` for traces storage,
** *(4)* It will utilize Elastic Search for persistence of traces (unlike  in the `dev-istio-system` namespace where `memory` is utilized)
** *(5)* The `jaeger-small-production` external `Jaeger` Resource is integrated by and utilized in the `Service Mesh`.

Login to the Openshift console with Mesh Operator credentials `emma/emma` and navigate to `user-$LAB_PARTICIPANT_ID-prod-istio-system` namespace. Verify all deployments and pods are running.

image::assets/03-travel-agency-expected-3-container-pods.png[500,10000]

NOTE: The configs came from link:../lab-3/create-prod-smcp-1-tracing.sh[create-prod-smcp-1-tracing.sh] script which you can inspect for details.

== Task 3: Add the Application Namespaces to the Production Mesh and create the Deployments

In this task we add the application namespaces to our newly created Service Mesh by specifying ServiceMeshMember resources and deploy the corresponding applications for production. We also configure the applications for the usage within the Service Mesh by specifying two `sidecar` containers:

1. `istio-proxy` sidecar container: used to proxy all communications in/out of the main application container and apply `Service Mesh` configurations
2. `jaeger-agent` sidecar container: The `Service Mesh` documentation link:https://docs.openshift.com/container-platform/4.11/service_mesh/v2x/ossm-reference-jaeger.html#distr-tracing-deployment-best-practices_jaeger-config-reference[Jaeger Agent Deployment Best Practices] mentions the options of deploying `jaeger-agent` as sidecar or as `DaemonSet`. We have selected the former in order to allow `multi-tenancy` in the Openshift cluster.

All application `Deployment`(s) will be patched as follows to include the sidecars (*Warning:* Don't apply as the script `deploy-travel-services-domain.sh` further down will do so):
----
oc patch deployment/voyages -p '{"metadata":{"annotations":{"sidecar.jaegertracing.io/inject": "jaeger-small-production"}}}' -n $ENV-travel-portal
oc patch deployment/voyages -p '{"spec":{"template":{"metadata":{"annotations":{"sidecar.istio.io/inject": "true"}}}}}' -n $ENV-travel-portal
----

Now let's get started.

* First we login as Mesh Developer with `farid/farid` who is responsible for the Travel Agency services and we check the Labels for the `user-$LAB_PARTICIPANT_ID-prod-travel-agency` application namespace
+
[source,shell]
----
./login-as.sh farid
./check-project-labels.sh user-$LAB_PARTICIPANT_ID-prod-travel-agency
----
+
The result of this command should look similar to this:
+
[source,shell]
----
{
  "kubernetes.io/metadata.name": "user-5-prod-travel-agency"
}
----

* Next we add the application namespaces to our Production Service Mesh Tenant and check the Labels again
+
[source,shell]
----
./create-membership.sh user-$LAB_PARTICIPANT_ID-prod-istio-system user-$LAB_PARTICIPANT_ID-production user-$LAB_PARTICIPANT_ID-prod-travel-agency

./check-project-labels.sh user-$LAB_PARTICIPANT_ID-prod-travel-agency
----
+
The result of this command should look similar to this (may need to retry a few times until all labels are applied):
+
[source,shell]
----
{
  "kiali.io/member-of": "user-5-prod-istio-system",
  "kubernetes.io/metadata.name": "user-5-prod-travel-agency",
  "maistra.io/member-of": "user-5-prod-istio-system"
}
----

* Now we deploy the Travel Agency Services applications and inject the sidecar containers.
+
[source,shell]
----
./deploy-travel-services-domain.sh prod prod-istio-system $LAB_PARTICIPANT_ID
----
+
You can also login with `farid/farid` in the Openshift Console and verify the applications created in your `user-$LAB_PARTICIPANT_ID-prod-travel-agency` namespace. It should look like:
+
image::assets/03-travel-agency-expected-3-container-pods.png[500,10000]


* In the next step we install the second part of our applications, the Travel Control and Travel Portal apps, with the responsible user `cristina/cristina`
+
[source,shell]
----
./login-as.sh cristina
./check-project-labels.sh user-$LAB_PARTICIPANT_ID-prod-travel-control
./check-project-labels.sh user-$LAB_PARTICIPANT_ID-prod-travel-portal
----

* Now we add the `user-$LAB_PARTICIPANT_ID-prod-travel-control` application namespace to the Mesh
+
[source,shell]
----
./create-membership.sh user-$LAB_PARTICIPANT_ID-prod-istio-system user-$LAB_PARTICIPANT_ID-production user-$LAB_PARTICIPANT_ID-prod-travel-control

./check-project-labels.sh user-$LAB_PARTICIPANT_ID-prod-travel-control
----

* Now we add the `user-$LAB_PARTICIPANT_ID-prod-travel-portal` application namespace to the Mesh
+
[source,shell]
----
./create-membership.sh user-$LAB_PARTICIPANT_ID-prod-istio-system user-$LAB_PARTICIPANT_ID-production user-$LAB_PARTICIPANT_ID-prod-travel-portal

./check-project-labels.sh user-$LAB_PARTICIPANT_ID-prod-travel-portal
----

* In the next step we are deploying the Travel Portal and Travel Control applications and injecting the sidecars.
+
[source,shell]
----
./deploy-travel-portal-domain.sh prod prod-istio-system $OCP_DOMAIN $LAB_PARTICIPANT_ID
----

* We can login with `cristina/cristina` in the Openshift Console and verify that the applications have been created and are running in the two namespaces:
** `user-$LAB_PARTICIPANT_ID-prod-travel-control`
+
image::assets/03-travel-control-expected-3-container-pods.png[300,700]

** `user-$LAB_PARTICIPANT_ID-prod-travel-portal`
+
image::assets/03-travel-portal-expected-3-container-pods.png[300,700]

== Task 4: Expose the Travel Portal Dashboard via TLS

Now after the deployment of the applications, we want to make them accessible outside of the cluster for the Travel Agency customers.
We also want to expose the services with a custom TLS cert.
In order to achieve that,

* we are going to create a TLS certificate
* store it in a secret in our SMCP namespace
* create on Openshift passthrough route forwarding traffic to the Istio ingress Gateway
* create an Istio Gateway Resource configured with our TLS certificate
* Right now if you login to the Kiali Dashboard with the user `emma/emma`, there is an issue in the `VirtualService` control and an error on Kiali as no `Gateway` exists yet.
+
image::./assets/03-no-gw-for-travel-control-ui-vs.png[400,800]

Now let's login as Mesh Operator with `emma/emma` and execute the described steps.

[source,shell]
----
./login-as.sh emma
./create-https-ingress-gateway.sh prod-istio-system $OCP_DOMAIN $LAB_PARTICIPANT_ID
----

NOTE: The configs come from link:../lab-3/create-https-ingress-gateway.sh[create-https-ingress-gateway.sh] script which you can inspect for details.

After finishing, the script above, you'll get the exposed URL Route and the `Travel Control Dashboard` should be accessible (look at the end of the script log for the URL) eg.:
https://travel-user-x.apps.cluster-vjzhs.vjzhs.sandbox1672.opentlc.com and the `Kiali` error on the link:./assets/03-corrected-gw-for-travel-control-ui-vs.png[`VirtualService` should also show as resolved].

image::assets/03-Travel-Control-Dashboard-https.png[Travel Control Dashboard]

== Task 5: Configure Prometheus for Production

In order to configure Prometheus for production, we have several options:

Option 1: Create a `PersistenceVolume` for the `SMCP` created `Prometheus` resource::
With this option the `mesh operator` will enhance the `SMCP` managed `Prometheus Deployment` resource in order to
* extend metric retention to 7 days (`7d`) and
* enable long-term persistence of the metrics by adding a persistent volume to the deployment.
Option 2: External `Prometheus` Setup via `prometheus-operator`::
With this option the `cluster admin` user will perform the following actions:
a. Deploy an additional `Prometheus Operator` in `prod-istio-system`
b. Deploy a `StatefulSet` based `Prometheus` resource with 2 replicas
c. Configure the prometheus replicas to monitor the components in `prod-istio-system` and all dataplane namespaces.
Option 3: Integrate with Openshift `Monitoring` Stack::
With this option only the `dataplane` metrics (`istio-proxy` and business container) are collected. These will be scraped by the Openshift Monitoring Stack's Prometheus and the changes required on the service mesh are described in link:https://access.redhat.com/solutions/6958679[How to configure user-workload to monitor ServiceMesh application in Openshift 4].
Option 4: Integrate with an external `Monitoring` Tool::
This option assumes that another tool like Datadog is used by the Operations team to collect metrics. In order to achieve this:

a. For `controlplane` components metrics collection, the tool needs to be part of the control plane namespace or a `NetworkPolicy` to allow it visibility to those components is required.
b. For `dataplane` metrics the same approach described, previously, in _Option 3_ is to be followed.

For the purpose of this lab we will deliver *Option 1* in the production setup. Login as `Mesh Operator` with `emma/emma`, create a PVC for Prometheus and update the Prometheus configuration.

[source,shell]
----
./login-as.sh emma
./update-prod-smcp-2-prometheus.sh user-$LAB_PARTICIPANT_ID-prod-istio-system
----

NOTE: The configs come from link:../lab-3/update-prod-smcp-2-prometheus.sh[update-prod-smcp-2-prometheus.sh] script which you can inspect for details.


== Task 6: Final Production Configuration

The following *Purpose* and *Principals* have been finalized with the `Travel Agency` architects and proposed `Service Mesh` configurations have been accepted based on these:

* *Purpose:*
** Secure service-to-service communications.
** Monitor usage and health of the inter-service communications.
** Allow separate teams to work in isolation whilst delivering parts of a solution.
* *Principals:*
** An external mechanism of configuration of traffic encryption, authentication and authorization.
** Transparent integration of additional services of expanding functionality.
** An external traffic management and orchestration mechanism.
** All components will be configured with High Availability in mind.
** Observability is to be used for verification of system "sound operation", not auditing.

Therefore, based on these rules and guidelines we will apply to the final `PROD` setup the following:

* _Tracing:_ used only for debug purposes (rather than as sensitive -auditing- information), so we choose to sample *5%* of all traces, whilst these are going to be stored for *7 Days*. Elastic Search cluster will be used for this long-term storage.
* _Metrics:_ will have long-term storage (**7 Days**) with further archiving of the metrics beyond this period in order to assist historical comparisons
* _Grafana:_ will have persistance storage
* _Istio Ingress/Egress Gateways:_  (scale up to 2 instances)
* _Istiod Controlplane_ (scale up to 2 instances)

We login as Mesh operator with 'emma/emma' and execute the final update script (on a separate command prompt execute `oc get pods -w -n user-$LAB_PARTICIPANT_ID-prod-istio-system` to follow the POD scalings).

[source,shell]
----
./login-as.sh emma
./update-prod-smcp-3-final.sh user-$LAB_PARTICIPANT_ID-prod-istio-system user-$LAB_PARTICIPANT_ID-production
----

NOTE: The configs come from link:../lab-3/update-prod-smcp-3-final.sh[update-prod-smcp-3-final.sh] script which you can inspect for details.

== Next Steps

IMPORTANT: *Before you move to Lab Scenario-4* inform the instructors you have completed this lab scenario as they will need to prepare the environment for the next lab.

Congratulations! You have helped the Travel Agency setup a production environment. You deserve a *5 minute break*! before moving to the next scenario.

link:scenario-4.adoc[Getting started with Scenario 4]
